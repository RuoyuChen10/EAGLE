{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0887455c",
   "metadata": {},
   "source": [
    "# ðŸ”¥ Qwen2.5-VL Model Attribution (Vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29daf716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cry/anaconda3/envs/qwen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the huggingface mirror and cache path\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\" # for Chinese\n",
    "os.environ[\"HF_HOME\"] = \"../model_checkpoint/hf_cache\"\n",
    "\n",
    "\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25700a54",
   "metadata": {},
   "source": [
    "### 1. Load Qwen2.5-VL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d827ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.06it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# default: Load the model on the available device(s)\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# default processor\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6d14a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"../demo.jpeg\",\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef16adf",
   "metadata": {},
   "source": [
    "### 2. Data Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3c3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,    # è¿™é‡Œå¯ä»¥å¤šä¸ª\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)    # dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b753e8",
   "metadata": {},
   "source": [
    "### 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2232c76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cry/anaconda3/envs/qwen/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Inference: Generation of the output\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **inputs, \n",
    "        do_sample=False,      # ç¦ç”¨é‡‡æ ·ï¼Œæ”¹ç”¨è´ªå©ªæœç´¢\n",
    "        num_beams=1,          # è®¾ç½®ä¸º1ï¼Œç¡®ä¿æ˜¯è´ªå©ªè€Œä¸æ˜¯beam search\n",
    "        max_new_tokens=128)\n",
    "    generated_ids_trimmed = [   # åŽ»æŽ‰å›¾åƒå’Œpromptçš„æ–‡æœ¬\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c098e2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The image depicts a serene beach scene with a person and a dog. The person is sitting on the sandy beach, facing the ocean, and appears to be interacting with the dog. The dog is also sitting on the sand, facing the person, and seems to be giving a paw or a friendly gesture. The person is wearing a plaid shirt and has long hair. The background shows the ocean with gentle waves and a clear sky, suggesting it might be early morning or late afternoon due to the soft lighting. The overall atmosphere of the image is calm and joyful, capturing a moment of connection between the person and their dog at the beach.']\n"
     ]
    }
   ],
   "source": [
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a8e3c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ä image', 'Ä depicts', 'Ä a', 'Ä serene', 'Ä beach', 'Ä scene', 'Ä with', 'Ä a', 'Ä person', 'Ä and', 'Ä a', 'Ä dog', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = processor.tokenizer\n",
    "tokens = tokenizer.convert_ids_to_tokens(generated_ids_trimmed[0])\n",
    "print(tokens[:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a7185e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  785,  2168, 61891,   264, 94763, 11321,  6109,   448,   264,  1697,\n",
       "          323,   264,  5562,    13], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids_trimmed[0][:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba9ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ä image', 'Ä depicts', 'Ä a', 'Ä serene', 'Ä beach', 'Ä scene', 'Ä with', 'Ä a', 'Ä person', 'Ä and', 'Ä a', 'Ä dog']\n"
     ]
    }
   ],
   "source": [
    "tokens_org = tokenizer.convert_ids_to_tokens(generated_ids[0])\n",
    "print(tokens_org[3602:3602+14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19733a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  785,  2168, 61891,   264, 94763, 11321,  6109,   448,   264,  1697,\n",
       "          323,   264,  5562], device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids[0][3602:3602+13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "275724b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3602"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cdc2feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  785,  2168, 61891,   264, 94763, 11321,  6109,   448,   264,  1697,\n",
       "          323,   264,  5562,    13,   576,  1697,   374, 11699,   389,   279,\n",
       "        67439, 11321,    11, 12880,   279, 17951,    11,   323,  7952,   311,\n",
       "          387, 44730,   448,   279,  5562,    13,   576,  5562,   374,  1083,\n",
       "        11699,   389,   279,  9278,    11, 12880,   279,  1697,    11,   323,\n",
       "         4977,   311,   387,  7086,   264, 76838,   476,   264, 11657, 30157,\n",
       "           13,   576,  1697,   374, 12233,   264,   625,  3779, 15478,   323,\n",
       "          702,  1293,  6869,    13,   576,  4004,  4933,   279, 17951,   448,\n",
       "        21700, 16876,   323,   264,  2797, 12884,    11, 22561,   432,  2578,\n",
       "          387,  4124,  6556,   476,  3309, 13354,  4152,   311,   279,  8413,\n",
       "        17716,    13,   576,  8084, 16566,   315,   279,  2168,   374, 19300,\n",
       "          323, 82112,    11, 39780,   264,  4445,   315,  3633,  1948,   279,\n",
       "         1697,   323,   862,  5562,   518,   279, 11321,    13],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids_trimmed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77864d",
   "metadata": {},
   "source": [
    "### 4. Get the target token probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac175298",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_new = inputs.copy()\n",
    "inputs_new['input_ids'] = generated_ids\n",
    "inputs_new['attention_mask'] = torch.ones_like(generated_ids)\n",
    "inputs_new = inputs_new.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa400f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward calculation to get all logits (including the logits of the input part)\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **inputs_new,\n",
    "        return_dict=True,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    all_logits = outputs.logits  # [batch_size, seq_len, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "091d8193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(785, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits[0,3601].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58d18699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(785, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids[0,3602]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "300d12df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids[0,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb23e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03341e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
